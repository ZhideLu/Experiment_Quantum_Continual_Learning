{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-bit quantum circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Yao, YaoPlots\n",
    "using LinearAlgebra, Statistics, Random, StatsBase, MAT, Printf\n",
    "using Flux: batch, Flux\n",
    "using Plots, PyPlot\n",
    "include(\"../functions/Function_QCL.jl\") ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 500 ;\n",
    "num_test = 100 ;\n",
    "num_qubit = 10 ;\n",
    "depth = 9 ;\n",
    "mid = Int(round(num_qubit/2)) ;\n",
    "dim = 2^num_qubit ;\n",
    "\n",
    "op0 = put(num_qubit, mid=>0.5*(I2+Z)) ;\n",
    "op1 = put(num_qubit, mid=>0.5*(I2-Z)) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = num_qubit * depth * 3 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 2 ;   # scaling factor\n",
    "data1 = matread(\"../dataset/MNIST_0-9.mat\")\n",
    "x_train_1 = data1[\"x_train\"] ;  y_train_1 = data1[\"y_train\"]\n",
    "x_test_1 = data1[\"x_test\"] ;  y_test_1 = data1[\"y_test\"] ;\n",
    "\n",
    "x_train_1 = real( x_train_1[ :, 1:num_train ] ) * c1\n",
    "y_train_1 = y_train_1[ 1 : num_train, : ] ;\n",
    "\n",
    "x_test_1 = real( x_test_1[ :, 1 : num_test] ) * c1\n",
    "y_test_1 = y_test_1[ 1 : num_test, : ] ;\n",
    "\n",
    "x_train_1_ = zeros(Float64,(dim,num_train))\n",
    "x_train_1_[1 : 256, :] = x_train_1\n",
    "x_train_1 = x_train_1_\n",
    "x_test_1_ = zeros(Float64,(dim,num_test))\n",
    "x_test_1_[1 : 256, :] = x_test_1\n",
    "x_test_1 = x_test_1_  ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ini_params  = [pi/6 for _ in 1 : dim] ;\n",
    "circuit = chain(chain(num_qubit, params_layer(num_qubit, 1:num_qubit), \n",
    "                   ent_cx(num_qubit, 1:num_qubit)) for _ in 1:depth) ;\n",
    "\n",
    "dispatch!(circuit, ini_params); \n",
    "YaoPlots.plot(circuit) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cir_1 = [chain( chain( num_qubit, params_layer(num_qubit, 1:num_qubit), \n",
    "              ent_cx(num_qubit, 1:num_qubit) ) for _ in 1 : depth ) for _ in 1 : num_train]\n",
    "test_cir_1 = [chain( chain( num_qubit, params_layer(num_qubit, 1:num_qubit), \n",
    "              ent_cx(num_qubit, 1:num_qubit) ) for _ in 1 : depth ) for _ in 1 : num_test] ;\n",
    "\n",
    "for i in 1 : num_train\n",
    "    dispatch!(train_cir_1[i], x_train_1[:,i]+ini_params)\n",
    "end\n",
    "for i in 1 : num_test\n",
    "    dispatch!(test_cir_1[i], x_test_1[:,i]+ini_params)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YaoPlots.plot(train_cir_1[2]) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = acc_loss_evaluation(num_qubit, test_cir_1, y_test_1, num_test, mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "batch_size = 25       # batch size\n",
    "lr1 = 0.02          # learning rate\n",
    "niters = 20      # number of iterations\n",
    "optim1 = Flux.ADAM(lr1) # Adam optimizer  \n",
    "\n",
    "# record the training history\n",
    "history_loss_train_1nd_1 = Float64[]\n",
    "history_acc_train_1nd_1 = Float64[]\n",
    "history_loss_test_1nd_1 = Float64[] ;\n",
    "history_acc_test_1nd_1 = Float64[] ;\n",
    "\n",
    "grad_1_history = [] ; \n",
    "\n",
    "para_1_history = [] ;\n",
    "distance_history = Float64[] ;\n",
    "\n",
    "para_1 = copy(ini_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in 1 : niters\n",
    "    # calculate the accuracy & loss for the training & test set\n",
    "#     acc_train_1nd_1, loss_train_1nd_1 = acc_loss_evaluation(num_qubit, train_cir_1, y_train_1, num_train, mid)\n",
    "    acc_test_1nd_1, loss_test_1nd_1 = acc_loss_evaluation(num_qubit, test_cir_1, y_test_1, num_test, mid)\n",
    "    \n",
    "#     push!(history_loss_train_1nd_1, loss_train_1nd_1 ) ;   push!(history_acc_train_1nd_1, acc_train_1nd_1) ;\n",
    "    push!(history_loss_test_1nd_1, loss_test_1nd_1) ;   push!(history_acc_test_1nd_1, acc_test_1nd_1)\n",
    "    push!(para_1_history, para_1)\n",
    "    push!(distance_history, norm(para_1 - ini_params ) )\n",
    "    \n",
    "    @printf(\"\\nStep=%d, test_loss=%.3f,test_acc=%.3f\\n\", k, loss_test_1nd_1, acc_test_1nd_1)\n",
    "    \n",
    "    # at each training epoch, randomly choose a batch of samples from the training set\n",
    "    batch_index = randperm(num_train)[1 : batch_size]\n",
    "    batch_cir = train_cir_1[batch_index]\n",
    "    y_batch = y_train_1[batch_index,:]\n",
    "\n",
    "    q_ = zeros(batch_size, 2);\n",
    "    for i=1 : batch_size\n",
    "        q_[i, :] = density_matrix(zero_state(num_qubit) |> batch_cir[i], (mid)) |> Yao.probs\n",
    "    end\n",
    "    \n",
    "    # calculate the gradients \n",
    "    Arr = Array{Float64}(zeros(batch_size, nparameters(batch_cir[1])))\n",
    "    for i in 1 : batch_size\n",
    "        Arr[i, :] = expect'(op0, zero_state(num_qubit)=>batch_cir[i])[2]\n",
    "    end\n",
    "    \n",
    "    C = [Arr, -Arr]\n",
    "    \n",
    "    grads = collect(mean([-sum([y_batch[i,j]*((1 ./ q_)[i,j])*batch(C)[i,:,j] for j in 1:2]) for i = 1 : batch_size]) )\n",
    "    push!(grad_1_history, copy(grads))\n",
    "    \n",
    "    # update the parameters\n",
    "    para_1 = Flux.Optimise.update!(optim1, copy(para_1), grads);\n",
    "    \n",
    "    # update the parameters\n",
    "    for i in 1 : num_train\n",
    "        dispatch!(train_cir_1[i], x_train_1[:,i] + para_1 )\n",
    "    end\n",
    "    for i in 1 : num_test\n",
    "        dispatch!(test_cir_1[i], x_test_1[:,i] + para_1 )\n",
    "    end\n",
    "    \n",
    "#     if ( acc_test_1nd_1 >= 0.97  &&  loss_test_1nd_1 <= 0.53 && k >= 10) || (k >= 25)\n",
    "#         break\n",
    "#     end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show(stdout, \"text/plain\", grad_1_history[6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm(para_1_history[20]- para_1_history[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots.plot(distance_history, label=\"distance_history\", legend = :bottomright) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fisher information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_loss_evaluation(num_qubit, test_cir_1, y_test_1, num_test, mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mean(real(fim1) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fim1 = fisher(num_train, train_cir_1, y_train_1) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_1 = grad_1_history[end] ; \n",
    "vector_1 = vector_1 / norm(vector_1) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = 2 ;\n",
    "# data2 = matread(\"./datasets/MedNIST_hand_breast_wk.mat\") \n",
    "data2 = matread(\"../dataset/FashionMNIST_0-9.mat\") \n",
    "\n",
    "num_train_2 = 500 ; num_test_2 = 100 ;\n",
    "\n",
    "x_train_2 = data2[\"x_train\"]\n",
    "y_train_2 = data2[\"y_train\"]\n",
    "x_test_2 = data2[\"x_test\"]\n",
    "y_test_2 = data2[\"y_test\"] ;\n",
    "\n",
    "x_train_2 = real( x_train_2[ :, 1 : num_train_2 ] ) * c2\n",
    "y_train_2 = y_train_2[ 1 : num_train_2, : ] ;\n",
    "\n",
    "x_test_2 = real( x_test_2[ :, 1 : num_test_2] ) * c2\n",
    "y_test_2 = y_test_2[ 1 : num_test_2, : ] ;\n",
    "\n",
    "x_train_2_ = zeros(Float64,(dim, num_train_2))\n",
    "x_train_2_[1:256,:] = x_train_2\n",
    "x_train_2 = x_train_2_\n",
    "x_test_2_ = zeros(Float64,(dim, num_test_2))\n",
    "x_test_2_[1:256,:] = x_test_2\n",
    "x_test_2 = x_test_2_  ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_acc_train_2nd_2 = Float64[] ;   history_acc_test_2nd_2 = Float64[] ;\n",
    "history_loss_train_2nd_2 = Float64[] ;   history_loss_test_2nd_2 = Float64[] ;\n",
    "\n",
    "history_acc_test_2nd_1 = Float64[] ;   history_loss_test_2nd_1 = Float64[] ;\n",
    "\n",
    "grad_2_history = [] ;\n",
    "\n",
    "para_2_history = [] ;\n",
    "distance_history_2 = Float64[] ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_2 = copy(para_1_history[end])  ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cir_2 = [chain(chain(num_qubit, params_layer(num_qubit,1:num_qubit), \n",
    "                    ent_cx(num_qubit,1:num_qubit)) for _ in 1:depth) for _ in 1 : num_train_2]\n",
    "test_cir_2 = [chain(chain(num_qubit, params_layer(num_qubit,1:num_qubit), \n",
    "                   ent_cx(num_qubit,1:num_qubit)) for _ in 1:depth) for _ in 1 : num_test_2];\n",
    "\n",
    "for i in 1 : num_train_2\n",
    "    dispatch!(train_cir_2[i], x_train_2[:,i]+para_2)\n",
    "end\n",
    "for i in 1 : num_test_2\n",
    "    dispatch!(test_cir_2[i], x_test_2[:,i]+para_2)\n",
    "end\n",
    "\n",
    "for i in 1 : num_train\n",
    "    dispatch!(train_cir_1[i], x_train_1[:,i]+para_2)\n",
    "end\n",
    "for i in 1 : num_test\n",
    "    dispatch!(test_cir_1[i], x_test_1[:,i]+para_2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "batch_size = 25       # batch size\n",
    "lr2 = 0.02         # learning rate\n",
    "niters = 20          # number of iterations\n",
    "optim2 = Flux.ADAM(lr2) ;# Adam optimizer  \n",
    "\n",
    "# lambda1 = 250;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in 1 : niters\n",
    "\n",
    "#     acc_train_2nd_2, loss_train_2nd_2 = acc_loss_evaluation(num_qubit, train_cir_2, y_train_2, num_train, mid)\n",
    "    acc_test_2nd_2, loss_test_2nd_2 = acc_loss_evaluation(num_qubit, test_cir_2, y_test_2, num_test_2, mid)\n",
    "    acc_test_2nd_1, loss_test_2nd_1 = acc_loss_evaluation(num_qubit, test_cir_1, y_test_1, num_test, mid)\n",
    "    \n",
    "#     push!(history_acc_train_2nd_2, acc_train_2nd_2) ;    push!(history_loss_train_2nd_2, loss_train_2nd_2) ;   \n",
    "    push!(history_acc_test_2nd_2, acc_test_2nd_2) ;    push!(history_loss_test_2nd_2, loss_test_2nd_2) ;   \n",
    "    \n",
    "    push!(history_acc_test_2nd_1, acc_test_2nd_1) ;    push!(history_loss_test_2nd_1, loss_test_2nd_1) ;  \n",
    "    \n",
    "    push!(para_2_history, para_2) ;     push!(distance_history_2, norm(para_2 - para_1_history[end] )    )  ;\n",
    "    \n",
    "    @printf(\"Step=%d, test_loss=%.3f,test_acc=%.3f\\n\", k, loss_test_2nd_2, acc_test_2nd_2)\n",
    "    @printf(\"task1, loss=%.3f, acc=%.3f\\n\", loss_test_2nd_1, acc_test_2nd_1)\n",
    "    \n",
    "    # at each training epoch, randomly choose a batch of samples from the training set\n",
    "    batch_index = randperm(num_train_2)[1 : batch_size]\n",
    "    batch_cir_2 = train_cir_2[batch_index]\n",
    "    y_batch_2 = y_train_2[batch_index, : ]\n",
    "\n",
    "    q_ = zeros(batch_size, 2);\n",
    "    for i = 1 : batch_size\n",
    "        q_[i, :] = density_matrix(zero_state(num_qubit) |> batch_cir_2[i], (mid)) |> Yao.probs\n",
    "    end\n",
    "    \n",
    "    # calculate the gradients w.r.t. the cross-entropy loss function\n",
    "    Arr = Array{Float64}(zeros(batch_size, nparameters(batch_cir_2[1])))\n",
    "    for i in 1 : batch_size\n",
    "        Arr[i, :] = expect'(op0, zero_state(num_qubit)=>batch_cir_2[i])[2]\n",
    "    end\n",
    "    \n",
    "    C = [Arr, -Arr]\n",
    "    \n",
    "    grads = collect(mean([-sum([y_batch_2[i,j]*((1 ./ q_)[i,j])*batch(C)[i,:,j] for j in 1 : 2]) for i=1 : batch_size]))\n",
    "    push!(grad_2_history, copy(grads) )\n",
    "    \n",
    "#     grads = grads + lambda1 * fim1 .* (para_2 - para_1)\n",
    "\n",
    "    \n",
    "    # update the parameters\n",
    "    updates = Flux.Optimise.update!(optim2, copy(para_2), grads);\n",
    "    para_2 = updates\n",
    "    \n",
    "    # update the parameters\n",
    "    for i in 1 : num_train_2\n",
    "        dispatch!(train_cir_2[i], x_train_2[:, i]+para_2)\n",
    "    end\n",
    "    for i in 1 : num_test_2\n",
    "        dispatch!(test_cir_2[i], x_test_2[:,i]+para_2)\n",
    "    end    \n",
    "    for i in 1 : num_test\n",
    "        dispatch!(test_cir_1[i], x_test_1[:,i]+para_2)\n",
    "    end       \n",
    "    \n",
    "#     if  (acc_test_2nd_2 >= 0.98 && loss_test_2nd_2 <= 0.58 && k >= 15) || (k>=20)\n",
    "#         break\n",
    "#     end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm(para_2_history[end]- para_2_history[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_task1 = vcat(history_acc_test_1nd_1, history_acc_test_2nd_1) ;\n",
    "acc_task2 = vcat(history_acc_test_2nd_2) ;\n",
    "length_1 = length(history_acc_test_1nd_1) ;\n",
    "length_2 = length(history_acc_test_2nd_1) ;\n",
    "length_ = [length_1, length_2] ;\n",
    "\n",
    "loss_task1 = vcat(history_loss_test_1nd_1, history_loss_test_2nd_1) ;\n",
    "loss_task2 = vcat(history_loss_test_2nd_2) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plot(acc_task1, color= :green, label = [\"task1: quantum data\"], marker=:o, markersize = 2, lw=2, \n",
    "       legend = :bottomright, ylabel=\"Accuracy\", xlabel=\"epochs\", right_margin = 1cm, left_margin = 0.1cm, top_margin = 0.1cm ) \n",
    "# Plots.plot!(acc_test_history_2, label=\"accuracy_test_time_of_flight\",legend = :bottomright) \n",
    "Plots.plot!(length_[1]+1 : sum(length_), acc_task2, color= :orange, marker=:o, markersize = 2, label = [\"task2: fashionmnist\"], \n",
    "      lw=2, legend = :bottomleft) \n",
    "p = Plots.twinx() ;\n",
    "Plots.plot!(p, loss_task1, color= :green, label = [\"task1: quantum data\"], marker=:star, markersize = 2, lw=2, size=(8*100, 5*100), \n",
    "                legend=:none, ylabel=\"Loss\") \n",
    "Plots.plot!(p, length_[1]+1 : sum(length_), loss_task2, color= :orange, marker=:star,  markersize = 2, label = [\"task2: quantum data\"], \n",
    "       lw=2, legend=:none, size=(8*100, 5*100) )\n",
    "\n",
    "size = 10\n",
    "Plots.plot!( xtickfontsize = size, ytickfontsize=size, xguidefontsize=size, yguidefontsize=size , \n",
    "                legendfontsize=size, titlefontsize = size, legendfont=font(7), framestyle=:box  ) \n",
    "\n",
    "# title!(\"10 qubits - learning two tasks sequantially\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matwrite(\"forgetting_10qubit-mnist&fashion.mat\", Dict(\n",
    "        \"acc_task1\" => acc_task1,\n",
    "        \"acc_task2\" => acc_task2,\n",
    "        \"length_\" => length_,\n",
    "        \"loss_task1\" => loss_task1, \n",
    "         \"loss_task2\" => loss_task2,\n",
    "    #\n",
    "         \"para_1_history\" => para_1_history,\n",
    "         \"grad_1_history\" => grad_1_history,\n",
    "         \"grad_2_history\" => grad_2_history,\n",
    "         \"para_2_history\" => para_2_history\n",
    ")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots.savefig(\"figures/continual learning for three tasks_15qubits.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0-rc1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
